{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHK5Tl2d2PFD","executionInfo":{"status":"ok","timestamp":1626539067048,"user_tz":-330,"elapsed":701,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}},"outputId":"c4a3cafe-7bc5-41c3-a490-065de4eafec3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bf50z5V42ZC4","executionInfo":{"status":"ok","timestamp":1626539067491,"user_tz":-330,"elapsed":22,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}},"outputId":"39999be4-2de3-4a09-a8f0-f4ae4c0856a2"},"source":["%cd /content/drive/MyDrive/Transformers/LanguageModeling/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Transformers/LanguageModeling\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LKx4d5XWFAgC"},"source":["**Masked language modeling**\n","For masked language modeling (MLM) we are going to use the same preprocessing as before for our dataset with one additional step: we will randomly mask some tokens (by replacing them by [MASK]) and the labels will be adjusted to only include the masked tokens (we don't have to predict the non-masked tokens)."]},{"cell_type":"code","metadata":{"id":"QQwgpex82OJF","executionInfo":{"status":"ok","timestamp":1626539067493,"user_tz":-330,"elapsed":18,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":[""],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"srJ0h1EfuLin","executionInfo":{"status":"ok","timestamp":1626539073192,"user_tz":-330,"elapsed":5716,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}},"outputId":"947a9a42-ba04-42c0-a2cf-e4c0cb707859"},"source":["!pip install transformers\n","import torch\n","!pip install datasets\n","from datasets import load_dataset\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","import pandas as pd\n","import numpy as np"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.9.0)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MgjENGS3uNtB","executionInfo":{"status":"ok","timestamp":1626539073933,"user_tz":-330,"elapsed":749,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}},"outputId":"78f5016d-72f7-4a9d-ed62-2d202d715926"},"source":["from datasets import load_dataset\n","datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Reusing dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zhFOg-CWuN61","executionInfo":{"status":"ok","timestamp":1626539075644,"user_tz":-330,"elapsed":1717,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["TrainData = pd.DataFrame(datasets['train'])\n","TestData = pd.DataFrame(datasets['test']['text'])\n","TestData.columns = [\"text\"]"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QNaLKEgsJ7qO","executionInfo":{"status":"ok","timestamp":1626539075655,"user_tz":-330,"elapsed":19,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}},"outputId":"f18950b2-cf37-437a-83de-98485a352ccf"},"source":["print(TrainData.shape,TestData.shape,sep=\"\\n\\n\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(36718, 1)\n","\n","(4358, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8j3B40hrJ73o","executionInfo":{"status":"ok","timestamp":1626539075656,"user_tz":-330,"elapsed":16,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["model_name = \"distilroberta-base\""],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"OAHAVqc6J8GN","executionInfo":{"status":"ok","timestamp":1626539075657,"user_tz":-330,"elapsed":16,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["from transformers import RobertaForMaskedLM,RobertaTokenizerFast"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8tNOkz7J8SE","executionInfo":{"status":"ok","timestamp":1626539082736,"user_tz":-330,"elapsed":7094,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n","Model = RobertaForMaskedLM.from_pretrained(model_name).to(device)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAtDej7LJ8ag","executionInfo":{"status":"ok","timestamp":1626539082740,"user_tz":-330,"elapsed":40,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["def tokenize_function(TrainData):\n","  input_idss = []\n","  attention_maskss = []\n","  for data in TrainData:\n","    encodings = tokenizer.encode_plus(data)\n","\n","    input_idss.append(encodings['input_ids'])\n","    attention_maskss.append(encodings['attention_mask'])\n","  \n","  return input_idss,attention_maskss"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TlS7PzZ2Mht7","executionInfo":{"status":"ok","timestamp":1626539091046,"user_tz":-330,"elapsed":8342,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}},"outputId":"67469af0-aff4-4652-ed11-fe90ab6f4114"},"source":["input_idsTrain,attention_maskTrain = tokenize_function(TrainData['text'])\n","input_idsTest,attention_maskTest = tokenize_function(TestData['text'])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"QkGqc6CKNLgU"},"source":["####so now we have not used padding over here , we can concatenate all the input_ids from all sentences & than \n","convert to one single list. than we will make that the lis of list having size (128) features..\n","####make the labels only for Masked token , because we want to train only the Masked token not all tokens."]},{"cell_type":"code","metadata":{"id":"EU6xrdUUMiC5","executionInfo":{"status":"ok","timestamp":1626539091050,"user_tz":-330,"elapsed":32,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["def Concatenate_fun(input_ids,attention_mask):\n","  input_ids = list(np.concatenate(np.array(input_ids)))\n","  attention_mask = list(np.concatenate(np.array(attention_mask)))\n","  return input_ids,attention_mask"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4RiicT-SNkng","executionInfo":{"status":"ok","timestamp":1626539091839,"user_tz":-330,"elapsed":817,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}},"outputId":"fe96edea-2cda-4f3d-ffbc-00a91d593770"},"source":["input_idsTrain,attention_maskTrain = Concatenate_fun(input_idsTrain,attention_maskTrain)\n","input_idsTest,attention_maskTest = Concatenate_fun(input_idsTest,attention_maskTest)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"geX8PT2DNnlE","executionInfo":{"status":"ok","timestamp":1626539091840,"user_tz":-330,"elapsed":12,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}},"outputId":"d3fd21d2-5dd1-41ad-d9f0-4261ec305aaf"},"source":["print(len(input_idsTrain),len(input_idsTest))\n","print(len(attention_maskTrain),len(attention_maskTest))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["2465320 292003\n","2465320 292003\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o3wWBiyGNq1m","executionInfo":{"status":"ok","timestamp":1626539091844,"user_tz":-330,"elapsed":11,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["def Grouping(lis,block_size):\n","  remainder = len(lis)%block_size\n","  idx = len(lis)-remainder\n","  lis_cut = lis[:idx] # chop-off excess data\n","  lis_new = [lis_cut[i:i+block_size] for i in range(0,len(lis_cut),block_size)]\n","  return lis_new"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcaV2QF3Nq9V","executionInfo":{"status":"ok","timestamp":1626539091846,"user_tz":-330,"elapsed":12,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["# block_size = tokenizer.model_max_length\n","block_size = 128"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-rgn3QSNvN0","executionInfo":{"status":"ok","timestamp":1626539092262,"user_tz":-330,"elapsed":427,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["labels_train = Grouping(input_idsTrain,128)           # making labels from input_ids\n","input_idsTrain_grouped = Grouping(input_idsTrain,128) # grouping input_ids in fix features\n","\n","labels_test = Grouping(input_idsTest,128)\n","input_idsTest_grouped = Grouping(input_idsTest,128)\n","\n","attention_maskTrain_grouped = Grouping(attention_maskTrain,128)\n","attention_maskTest_grouped = Grouping(attention_maskTest,128)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_I8empBNx0O","executionInfo":{"status":"ok","timestamp":1626539093024,"user_tz":-330,"elapsed":13,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["trainDict = {\n","    \"input_ids\":torch.tensor(input_idsTrain_grouped,dtype=torch.long),\n","    \"attention_mask\":torch.tensor(attention_maskTrain_grouped,dtype=torch.long),\n","    \"labels\":torch.tensor(labels_train,dtype=torch.long)\n","    }\n","\n","testDict = {\n","    \"input_ids\":torch.tensor(input_idsTest_grouped,dtype=torch.long),\n","    \"attention_mask\":torch.tensor(attention_maskTest_grouped,dtype=torch.long),\n","    \"labels\":torch.tensor(labels_test,dtype=torch.long)\n","    }"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"wkCHtZ_xN1u2","executionInfo":{"status":"ok","timestamp":1626539093026,"user_tz":-330,"elapsed":14,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["class CustomDataset(torch.utils.data.Dataset):\n","  def __init__(self,dictionary):\n","    self.dic = dictionary\n","    self.id = self.dic['input_ids']\n","    self.mask = self.dic['attention_mask']\n","    self.label = self.dic['labels']\n","    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","  def __len__(self):\n","    return len(self.label)\n","\n","  def __getitem__(self,index):\n","    self.input_ids = self.id[index]\n","    self.attention_mask = self.mask[index]\n","    self.labels = self.label[index]\n","    \n","    dict_map = {\n","        \"input_ids\":self.input_ids,\n","        \"attention_mask\":self.attention_mask,\n","        \"labels\":self.labels\n","        }\n","\n","    return dict_map"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"1V5NuW8LN3-r","executionInfo":{"status":"ok","timestamp":1626539093027,"user_tz":-330,"elapsed":13,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["Train_iter = CustomDataset(trainDict)\n","Test_iter = CustomDataset(testDict)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"XovRKlIuN6kA","executionInfo":{"status":"ok","timestamp":1626539095128,"user_tz":-330,"elapsed":2113,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["from transformers import Trainer, TrainingArguments,DataCollatorForLanguageModeling"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"baiJkZ45OCHl","executionInfo":{"status":"ok","timestamp":1626539095136,"user_tz":-330,"elapsed":31,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["# Finally, we use a special data_collator. \n","# The data_collator is a function that is responsible of taking the samples and batching them in tensors. \n","# In the previous example, we had nothing special to do, so we just used the default for this argument. \n","# Here we want to do the random-masking. We could do it as a pre-processing step (like the tokenization) \n","# but then the tokens would always be masked the same way at each epoch. By doing this step inside the data_collator, \n","# we ensure this random masking is done in a new way each time we go over the data."],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xzbq9j5EQJ7f","executionInfo":{"status":"ok","timestamp":1626539095137,"user_tz":-330,"elapsed":28,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True ,mlm_probability=0.15)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"t6ck_baOPPt1","executionInfo":{"status":"ok","timestamp":1626539095140,"user_tz":-330,"elapsed":30,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["training_args = TrainingArguments(output_dir=\"OutputDir\",\n","    do_train = True,\n","    do_eval=True,\n","    do_predict=True,\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    weight_decay=0.01,\n","    num_train_epochs=5.0,\n","    logging_dir = \"Logs/\",\n","    logging_strategy = \"epoch\",\n","    per_device_train_batch_size = 4,\n","    per_device_eval_batch_size = 2,\n","    save_strategy = \"epoch\",\n","    )"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"75BjRw7bPrO3","executionInfo":{"status":"ok","timestamp":1626539095142,"user_tz":-330,"elapsed":30,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["trainer = Trainer(\n","    model=Model,\n","    args=training_args,\n","    train_dataset=Train_iter,\n","    eval_dataset=Test_iter,\n","    data_collator = data_collator\n",")"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"WSwsvqheSFs0","executionInfo":{"status":"ok","timestamp":1626539095143,"user_tz":-330,"elapsed":29,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["# function ClickConnect(){\n","# console.log(\"Working\"); \n","# document.querySelector(\"colab-toolbar-button#connect\").click() \n","# }\n","# setInterval(ClickConnect,60000)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"QHcnCWlVQtOo","executionInfo":{"status":"ok","timestamp":1626541988897,"user_tz":-330,"elapsed":111746,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}},"outputId":"74e555b3-0ca3-4339-deed-93a00da42dcc"},"source":["trainer.train()"],"execution_count":27,"outputs":[{"output_type":"stream","text":["***** Running training *****\n","  Num examples = 19260\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 24075\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='23308' max='24075' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [23308/24075 46:21 < 01:31, 8.38 it/s, Epoch 4.84/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.149800</td>\n","      <td>1.896548</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.009100</td>\n","      <td>1.849470</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.938200</td>\n","      <td>1.822010</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.894700</td>\n","      <td>1.841298</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 2281\n","  Batch size = 2\n","Saving model checkpoint to OutputDir/checkpoint-4815\n","Configuration saved in OutputDir/checkpoint-4815/config.json\n","Model weights saved in OutputDir/checkpoint-4815/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 2281\n","  Batch size = 2\n","Saving model checkpoint to OutputDir/checkpoint-9630\n","Configuration saved in OutputDir/checkpoint-9630/config.json\n","Model weights saved in OutputDir/checkpoint-9630/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 2281\n","  Batch size = 2\n","Saving model checkpoint to OutputDir/checkpoint-14445\n","Configuration saved in OutputDir/checkpoint-14445/config.json\n","Model weights saved in OutputDir/checkpoint-14445/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 2281\n","  Batch size = 2\n","Saving model checkpoint to OutputDir/checkpoint-19260\n","Configuration saved in OutputDir/checkpoint-19260/config.json\n","Model weights saved in OutputDir/checkpoint-19260/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='24075' max='24075' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [24075/24075 48:13, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.149800</td>\n","      <td>1.896548</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.009100</td>\n","      <td>1.849470</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.938200</td>\n","      <td>1.822010</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.894700</td>\n","      <td>1.841298</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.859000</td>\n","      <td>1.803294</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 2281\n","  Batch size = 2\n","Saving model checkpoint to OutputDir/checkpoint-24075\n","Configuration saved in OutputDir/checkpoint-24075/config.json\n","Model weights saved in OutputDir/checkpoint-24075/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=24075, training_loss=1.970149191978193, metrics={'train_runtime': 2893.738, 'train_samples_per_second': 33.279, 'train_steps_per_second': 8.32, 'total_flos': 6077176593638400.0, 'train_loss': 1.970149191978193, 'epoch': 5.0})"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0fLmda4SKls","executionInfo":{"status":"ok","timestamp":1626541989946,"user_tz":-330,"elapsed":1060,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}},"outputId":"888015ec-9f04-4136-e2f1-e32f4e8224b9"},"source":["tokenizer.save_pretrained(\"Tokenizer/\")"],"execution_count":28,"outputs":[{"output_type":"stream","text":["tokenizer config file saved in Tokenizer/tokenizer_config.json\n","Special tokens file saved in Tokenizer/special_tokens_map.json\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["('Tokenizer/tokenizer_config.json',\n"," 'Tokenizer/special_tokens_map.json',\n"," 'Tokenizer/vocab.json',\n"," 'Tokenizer/merges.txt',\n"," 'Tokenizer/added_tokens.json',\n"," 'Tokenizer/tokenizer.json')"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"FaW8qEAGZXps"},"source":["## Prediction"]},{"cell_type":"code","metadata":{"id":"8fN1J2eNCqk7","executionInfo":{"status":"ok","timestamp":1626542027295,"user_tz":-330,"elapsed":470,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["from transformers import RobertaForMaskedLM,RobertaTokenizerFast"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Unhqnv8cPip9","executionInfo":{"status":"ok","timestamp":1626542070407,"user_tz":-330,"elapsed":554,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}},"outputId":"4d741945-148e-45ff-aacc-28c47cdce9c2"},"source":["!ls"],"execution_count":36,"outputs":[{"output_type":"stream","text":["checkpoint-24075  Logs\tMLM.ipynb  OutputDir  Tokenizer\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XSGG-Y4tCrZi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626542108610,"user_tz":-330,"elapsed":2196,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}},"outputId":"fc205b48-0768-4dba-8fcf-e0c979f44f41"},"source":["Model = RobertaForMaskedLM.from_pretrained(\"ModelDir/\")\n","tokenizer = RobertaTokenizerFast.from_pretrained(\"/content/drive/MyDrive/Transformers/LanguageModeling/Tokenizer/\")"],"execution_count":37,"outputs":[{"output_type":"stream","text":["loading configuration file ModelDir/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"distilroberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.8.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file ModelDir/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForMaskedLM.\n","\n","All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ModelDir/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n","Didn't find file /content/drive/MyDrive/Transformers/LanguageModeling/Tokenizer/added_tokens.json. We won't load it.\n","loading file /content/drive/MyDrive/Transformers/LanguageModeling/Tokenizer/vocab.json\n","loading file /content/drive/MyDrive/Transformers/LanguageModeling/Tokenizer/merges.txt\n","loading file /content/drive/MyDrive/Transformers/LanguageModeling/Tokenizer/tokenizer.json\n","loading file None\n","loading file /content/drive/MyDrive/Transformers/LanguageModeling/Tokenizer/special_tokens_map.json\n","loading file /content/drive/MyDrive/Transformers/LanguageModeling/Tokenizer/tokenizer_config.json\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qj3NmBs7ZWlb","executionInfo":{"status":"ok","timestamp":1626542125940,"user_tz":-330,"elapsed":726,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}},"outputId":"b988bfb7-7954-40a4-84dc-e2f80aa0ad38"},"source":["sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n","\n","\n","# encoding the text via tokenizer\n","# input = tokenizer.encode(sequence, return_tensors=\"pt\",return_attention_mask=True) \n","\n","# encoding the text via tokenizer without attention mask ,both fine\n","input = tokenizer.encode(sequence, return_tensors=\"pt\",return_attention_mask=False) \n","\n","\n","print(\"Length of input tokens : \"+str(len(input[0])))\n","\n","# getting index of mask_token_id\n","mask_token_index = torch.where(input == tokenizer.mask_token_id)[1] \n","print(mask_token_index)\n","print(\"out of 27 token , mask token index is : \"+str(mask_token_index))\n","Model.to(\"cpu\")\n","token_logits = Model(input).logits\n","\n","\n","mask_token_logits = token_logits[0, mask_token_index, :] # selct prob of only mask token out of 27 token\n","\n","# Two Ways of prediction ==> \n","# 1. top 5 higher prob. tokens\n","print(\"\\nTop 5 predictions\\n\")\n","top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n","for token in top_5_tokens:\n","  print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))\n","\n","print(\"\\n\\n\\n\")\n","# 2 . Only one Higher probability token\n","print(\"Only One Prediction\")\n","predict_token_id = torch.argmax(mask_token_logits,dim=1)\n","print(sequence.replace(tokenizer.mask_token,tokenizer.decode(predict_token_id)))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Length of input tokens : 27\n","tensor([21])\n","out of 27 token , mask token index is : tensor([21])\n","\n","Top 5 predictions\n","\n","Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  reduce our carbon footprint.\n","Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  lower our carbon footprint.\n","Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  decrease our carbon footprint.\n","Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  cut our carbon footprint.\n","Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  minimize our carbon footprint.\n","\n","\n","\n","\n","Only One Prediction\n","Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  reduce our carbon footprint.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1z1sKgzCZWqk","executionInfo":{"status":"aborted","timestamp":1626541991461,"user_tz":-330,"elapsed":2,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lp4D3cy_hSwN"},"source":["##Model quantization"]},{"cell_type":"code","metadata":{"id":"ZErybqWbaKEr","executionInfo":{"status":"ok","timestamp":1626542135938,"user_tz":-330,"elapsed":1837,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["quantized_model = torch.quantization.quantize_dynamic(\n","    Model, {torch.nn.Linear}, dtype=torch.qint8\n",")"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"8VoG5EWOhsmu","executionInfo":{"status":"ok","timestamp":1626542137745,"user_tz":-330,"elapsed":9,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["out = quantized_model(input)['logits']"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-yBCqgDPhxrX","executionInfo":{"status":"ok","timestamp":1626542140826,"user_tz":-330,"elapsed":466,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}},"outputId":"ed769acb-17e4-41bc-a94e-ee7ce1ec2024"},"source":["mask_token_logits = out[0,mask_token_index,:]\n","\n","predict_token_id = torch.argmax(mask_token_logits,dim=1)\n","print(sequence.replace(tokenizer.mask_token,tokenizer.decode(predict_token_id)))"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  reduce our carbon footprint.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BAcb8BFWiJlk","executionInfo":{"status":"ok","timestamp":1626542145249,"user_tz":-330,"elapsed":2123,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["# save qantized model\n","torch.save(quantized_model,\"QuantizedMLM.pt\")"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8uXlFd_iUJl","executionInfo":{"status":"ok","timestamp":1626542193533,"user_tz":-330,"elapsed":1308,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":["QLoaded = torch.load(\"QuantizedMLM.pt\")"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZAnJCy1ICjH1","executionInfo":{"status":"aborted","timestamp":1626541991463,"user_tz":-330,"elapsed":4,"user":{"displayName":"sarang tamrakar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiygD8soHFHwnL50GjUAlsd7jf8mnM-DDZWG-pqb38=s64","userId":"13053906188370509008"}}},"source":[""],"execution_count":null,"outputs":[]}]}